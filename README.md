# AI Embeddings

**Theoretical Foundation**:

In artificial intelligence (AI), embeddings are mathematical representations of objects like text, images, and audio that are designed to be consumed by machine learning models and semantic search algorithms.

Embeddings translate these objects into a mathematical form according to the factors or traits each one may or may not have, and the categories they belong to1. Technically, embeddings are vectors created by machine learning models for the purpose of capturing meaningful data about each object.

The distance between two vectors in this embedding space measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness2. This makes it possible for computers to understand the relationships between words and other objects, which is foundational for AI.

For example, in natural language processing (NLP), word embeddings are used to capture semantic meanings of words. Words that are used and occur in the same contexts tend to have similar meanings. By training on a large amount of text data, word embeddings can capture these similarities in a dense vector format, where each word is represented by a vector in high-dimensional space.

Embeddings are commonly used for various tasks such as search (where results are ranked by relevance to a query string), clustering (where text strings are grouped by similarity), recommendations (where items with related text strings are recommended), anomaly detection (where outliers with little relatedness are identified), diversity measurement (where similarity distributions are analyzed), and classification (where text strings are classified by their most similar label).

**A way to match your resume with a job in a multi-job listing**:

![](https://github.com/alex-peter-george/ai-embeddings-samples/blob/main/assets/embeddings-diagram.jpg)

